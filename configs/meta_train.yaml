# Meta-Training Configuration

algorithm: "maml" 

meta_learning:
  num_iterations: 1000
  
  meta_batch_size: 2
  
  outer_lr: 0.001
  
  inner_lr: 0.01
  
  num_inner_steps: 5
  
  num_adaptation_episodes: 5
  
  num_meta_episodes: 10

maml:
  first_order: true
  
  max_grad_norm: 1.0

rl2:
  hidden_size: 256
  
  episodes_per_trial: 5

policy_gradient:
  gamma: 0.99
  
  gae_lambda: 0.95
  
  entropy_coef: 0.01
  
  value_coef: 0.5
  
  normalize_advantages: true

agent:
  encoder_type: "tinybert"
  
  tinybert:
    model_name: "huawei-noah/TinyBERT_General_4L_312D"
    freeze_layers: 2  # TinyBERT has 4 layers, freeze first 2
    hidden_size: 312
  
  policy:
    hidden_sizes: [256, 128]
    activation: "relu"
  
  value:
    hidden_sizes: [256, 128]
    activation: "relu"

training:
  val_every: 50
  
  save_every: 100
  
  num_val_tasks: 10
  
  patience: 20

logging:
  log_dir: "logs"
  tensorboard: true
  log_every: 10
  
checkpoints:
  checkpoint_dir: "checkpoints"
  keep_best: 3

seed: 42

device: "auto"
